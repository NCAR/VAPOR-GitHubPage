<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>The MPI version of DART &mdash; DART 2.0.0 documentation</title>
    
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '2.0.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="DART 2.0.0 documentation" href="../index.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="the-mpi-version-of-dart">
<h1>The MPI version of DART<a class="headerlink" href="#the-mpi-version-of-dart" title="Permalink to this headline">¶</a></h1>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>&nbsp;</td>
<td><div class="first last line-block">
<div class="line">Jump to <a class="reference external" href="../index.html">DART Documentation Main
Index</a></div>
<div class="line">version information for this file:</div>
<div class="line">$Id$</div>
</div>
</td>
</tr>
</tbody>
</table>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<div class="line-block">
<div class="line">The latest release of the DART system includes an MPI option. MPI
stands for &#8216;Message Passing Interface&#8217;, and is both a library and
run-time system that enables multiple copies of a single program to
run in parallel, exchange data, and combine to solve a problem more
quickly. The latest release of DART does *NOT* require MPI to run;
the default build scripts do not need nor use MPI in any way. However,
for larger models with large state vectors and large numbers of
observations, the data assimilation step will run much faster in
parallel, which requires MPI to be installed and used. However, if
multiple ensembles of your model fit comfortably (in time and memory
space) on a single processor, you need read no further about MPI.</div>
<div class="line">MPI is an open-source standard; there are many implementations of it.
If you have a large single-vendor system it probably comes with an MPI
library by default. For a Linux cluster there are generally more
variations in what might be installed; most systems use a version of
MPI called MPICH. In smaller clusters or dual-processor workstations a
version of MPI called either LAM-MPI or OpenMPI might be installed, or
can be downloaded and installed by the end user. (Note that OpenMP is
a different parallel system; OpenMPI is a recent effort with a
confusingly similar name.)</div>
<div class="line">An &#8220;MPI program&#8221; makes calls to an MPI library, and needs to be
compiled with MPI include files and libraries. Generally the MPI
installation includes a shell script called &#8216;mpif90&#8217; which adds the
flags and libraries appropriate for each type of fortran compiler. So
compiling an MPI program usually means simply changing the fortran
compiler name to the MPI script name.</div>
<div class="line">These MPI scripts are built during the MPI install process and are
specific to a particular compiler; if your system has multiple fortran
compilers installed then either there will be multiple MPI scripts
built, one for each compiler type, or there will be an environment
variable or flag to the MPI script to select which compiler to invoke.
See your system documentation or find an example of a successful MPI
program compile command and copy it.</div>
</div>
</div>
<div class="section" id="dart-use-of-mpi">
<h2>DART use of MPI<a class="headerlink" href="#dart-use-of-mpi" title="Permalink to this headline">¶</a></h2>
<div class="line-block">
<div class="line">To run in parallel, only the DART &#8216;filter&#8217; program (and possibly the
companion &#8216;wakeup_filter&#8217; program) need be compiled with the MPI
scripts. All other DART executables should be compiled with a standard
F90 compiler and are not MPI enabled. (And note again that &#8216;filter&#8217;
can still be built as a single executable like previous releases of
DART; using MPI and running in parallel is simply an additional
option.) To build a parallel version of the &#8216;filter&#8217; program, the
&#8216;mkmf_filter&#8217; command needs to be called with the &#8216;-mpi&#8217; option to
generate a Makefile which compiles with the MPI scripts instead of the
Fortran compiler.</div>
<div class="line">See the <em>quickbuild.csh</em> script in each $DART/models/*/work directory
for the commands that need to be edited to enable the MPI utilities.
You will also need to edit the $DART/mkmf/mkmf.template file to call
the proper version of the MPI compile script if it does not have the
default name, is not in a standard location on the system, or needs
additional options set to select between multiple Fortran compilers.</div>
<div class="line">MPI programs generally need to be started with a shell script called
&#8216;mpirun&#8217; or &#8216;mpiexec&#8217;, but they also interact with any batch control
system that might be installed on the cluster or parallel system.
Parallel systems with multiple users generally run some sort of batch
system (e.g. LSF, PBS, POE, LoadLeveler, etc). You submit a job
request to this system and it schedules which nodes are assigned to
which jobs. Unfortunately the details of this vary widely from system
to system; consult your local web pages or knowledgeable system admin
for help here. Generally the run scripts supplied with DART have
generic sections to deal with LSF, PBS, no batch system at all, and
sequential execution, but the details (e.g. the specific queue names,
accounting charge codes) will almost certainly have to be adjusted.</div>
<div class="line">The data assimilation process involves running multiple copies
(ensembles) of a user model, with an assimilation computation
interspersed between calls to the model. There are many possible
execution combinations, including:</div>
</div>
<ul class="simple">
<li>Compiling the assimilation program &#8216;filter&#8217; with the model, resulting
in a single executable. This can be either a sequential or parallel
program.</li>
<li>Compiling &#8216;filter&#8217; separately from the model, and having 2 separate
executables. Either or both can be sequential or parallel.</li>
</ul>
<div class="line-block">
<div class="line">The choice of how to combine the &#8216;filter&#8217; program and the model has 2
parts: building the executables and then running them. At build time,
the choice of using MPI or not must be made. At execution time, the
setting of the &#8216;async&#8217; namelist value in the filter_nml section
controls how the &#8216;filter&#8217; program interacts with the model.</div>
<div class="line">Choices include:</div>
</div>
<ul class="simple">
<li>async = 0
The model and filter programs are compiled into a single executable,
and when the model needs to advance, the filter program calls a
subroutine. See a <a class="reference external" href="filter_async_modes.html#async0">diagram</a> which
illustrates this option.</li>
<li>async = 2</li>
</ul>
<blockquote>
<div>The model is compiled into a sequential (single task) program. If
&#8216;filter&#8217; is running in parallel, each filter task will execute the
model independently to advance the group of ensembles. See a
<a class="reference external" href="filter_async_modes.html#async2">diagram</a> which illustrates this
option.</div></blockquote>
<ul class="simple">
<li>async = 4</li>
</ul>
<blockquote>
<div>The model is compiled into an MPI program (parallel) and only
&#8216;filter&#8217; task 0 tells the startup script when it is time to advance
the model. Each ensemble is advanced one by one, with the model using
all the processors to run in parallel. See a
<a class="reference external" href="filter_async_modes.html#async4">diagram</a> which illustrates this
option.</div></blockquote>
<ul class="simple">
<li>async ignored (sometimes referred to as &#8216;async 5&#8217;, but not a setting</li>
</ul>
<blockquote>
<div>in the namelist)
This is the way most large models run now. There is a separate
script, outside of filter, which runs the N copies of the model to do
the advance. Then filter is run, as an MPI program, and it only
assimilates for a single time and then exits. The external script
manages the file motion between steps, and calls both the models and
filter in turn.</div></blockquote>
<div class="line-block">
<div class="line">This release of DART has the restriction that if the model and the
&#8216;filter&#8217; program are both compiled with MPI and are run in &#8216;async=4&#8217;
mode, that they both run on the same number of processors; e.g. if
&#8216;filter&#8217; is run on 16 processors, the model must be started on 16
processors as well. Alternatively, if the user model is compiled as a
single executable (async=2), &#8216;filter&#8217; can run in parallel on any
number of processors and each model advance can be executed
independently without the model having to know about MPI or
parallelism.</div>
<div class="line">Compiling and running an MPI application can be substantially more
complicated than running a single executable. There are a suite of
small test programs to help diagnose any problems encountered in
trying to run the new version of DART. Look in
<a class="reference external" href="../../developer_tests/mpi_utilities/tests/README">developer_tests/mpi_utilities/tests/README</a>
for instructions and a set of tests to narrow down any difficulties.</div>
</div>
</div>
<div class="section" id="performance-issues-and-timing-results">
<h2>Performance issues and timing results<a class="headerlink" href="#performance-issues-and-timing-results" title="Permalink to this headline">¶</a></h2>
<p>Getting good performance from a parallel program is frequently
difficult. Here are a few of reasons why:</p>
<ul class="simple">
<li>Amdahl&#8217;s law
You can look up the actual formula for this &#8220;law&#8221; in the Wikipedia,
but the gist is that the amount of serial code in your program limits
how much faster your program runs on a parallel machine, and at some
point (often much sooner than you&#8217;d expect) you stop getting any
speedup when adding more processors.</li>
<li>Surface area to volume ratio
Many scientific problems involve breaking up a large grid or array of
data and distributing the smaller chunks across the multiple
processors. Each processor computes values for the data on the
interior of the chunk they are given, but frequently the data along
the edges of each chunk must be communicated to the processors which
hold the neighboring chunks of the grid. As you increase the number
of processors (and keep the problem size the same) the chunk size
becomes smaller. As this happens, the &#8216;surface area&#8217; around the edges
decreases slower than the &#8216;volume&#8217; inside that one processor can
compute independently of other processors. At some point the
communication overhead of exchanging edge data limits your speedup.</li>
<li>Hardware architecture system balance
Raw CPU speeds have increased faster than memory access times, which
have increased faster than access to secondary storage (e.g. I/O to
disk). Computations which need to read input data and write result
files typically create I/O bottlenecks. There are machines with
parallel filesystems, but many programs are written to have a single
processor read in the data and broadcast it to all the other
processors, and collect the data on a single node before writing. As
the number of processors increases the amount of time spent waiting
for I/O and communication to and from the I/O node increases. There
are also capacity issues; for example the amount of memory available
on the I/O node to hold the entire dataset can be insufficient.</li>
<li>NUMA memory
Many machines today have multiple levels of memory: on-chip private
cache, on-chip shared cache, local shared memory, and remote shared
memory. The approach is referred as Non-Uniform Memory Access (NUMA)
because each level of memory has different access times. While in
general having faster memory improves performance, it also makes the
performance very difficult to predict since it depends not just on
the algorithms in the code, but is very strongly a function of
working-set size and memory access patterns. Beyond shared memory
there is distributed memory, meaning multiple CPUs are closely
connected but cannot directly address the other memory. The
communication time between nodes then depends on a hardware switch or
network card, which is much slower than local access to memory. The
performance results can be heavily influenced in this case by problem
size and amount of communication between processes.</li>
</ul>
<p>Parallel performance can be measured and expressed in several different
ways. A few of the relevant definitions are:</p>
<ul class="simple">
<li>Speedup
Generally defined as the wall-clock time for a single processor
divided by the wall-clock time for N processors.</li>
<li>Efficiency
The speedup number divided by N, which for perfect scalability will
remain at 1.0 as N increases.</li>
<li>Strong scaling
The problem size is held constant and the number of processors is
increased.</li>
<li>Weak scaling
The problem size grows as the number of processors increases so the
amount of work per processor is held constant.</li>
</ul>
<p>We measured the strong scaling efficiency of the DART &#8216;filter&#8217; program
on a variety of platforms and problem sizes. The scaling looks very good
up to the numbers of processors available to us to test on. It is
assumed that for MPP (Massively-Parallel Processing) machines with
10,000s of processors that some algorithmic changes will be required.
These are described in <a class="reference external" href="http://www.image.ucar.edu/DAReS/DART/scalable_paper.pdf">this
paper</a>.</p>
</div>
<div class="section" id="user-considerations-for-their-own-configurations">
<h2>User considerations for their own configurations<a class="headerlink" href="#user-considerations-for-their-own-configurations" title="Permalink to this headline">¶</a></h2>
<div class="line-block">
<div class="line">Many parallel machines today are a hybrid of shared and distributed
memory processors; meaning that some small number (e.g. 2-32) of CPUs
share some amount of physical memory and can transfer data quickly
between them, while communicating data to other CPUs involves slower
communication across either some kind of hardware switch or fabric, or
a network communication card like high speed ethernet.</div>
<div class="line">Running as many tasks per node as CPUs per shared-memory node is in
general good, unless the total amount of virtual memory used by the
program exceeds the physical memory. Factors to consider here include
whether each task is limited by the operating system to 1/Nth of the
physical memory, or whether one task is free to consume more than its
share. If the node starts paging memory to disk, performance takes a
huge nosedive.</div>
<div class="line">Some models have large memory footprints, and it may be necessary to
run in MPI mode not necessarily because the computation is faster in
parallel, but because the dataset size is larger than the physical
memory on a node and must be divided and spread across multiple nodes
to avoid paging to disk.</div>
</div>
<hr class="docutils" />
<div class="section" id="terms-of-use">
<h3>Terms of Use<a class="headerlink" href="#terms-of-use" title="Permalink to this headline">¶</a></h3>
<p>DART software - Copyright UCAR. This open source software is provided by
UCAR, &#8220;as is&#8221;, without charge, subject to all terms of use at
<a class="reference external" href="http://www.image.ucar.edu/DAReS/DART/DART_download">http://www.image.ucar.edu/DAReS/DART/DART_download</a></p>
<table border="1" class="docutils">
<colgroup>
<col width="39%" />
<col width="61%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Contact:</td>
<td>Nancy Collins</td>
</tr>
<tr class="row-even"><td>Revision:</td>
<td>$Revision$</td>
</tr>
<tr class="row-odd"><td>Source:</td>
<td>$URL$</td>
</tr>
<tr class="row-even"><td>Change Date:</td>
<td>$Date$</td>
</tr>
<tr class="row-odd"><td>Change&nbsp;history:</td>
<td>try &#8220;svn&nbsp;log&#8221; or &#8220;svn&nbsp;diff&#8221;</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">The MPI version of DART</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#dart-use-of-mpi">DART use of MPI</a></li>
<li><a class="reference internal" href="#performance-issues-and-timing-results">Performance issues and timing results</a></li>
<li><a class="reference internal" href="#user-considerations-for-their-own-configurations">User considerations for their own configurations</a><ul>
<li><a class="reference internal" href="#terms-of-use">Terms of Use</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/docs/mpi_intro.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, DART Team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.4.4</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/docs/mpi_intro.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>